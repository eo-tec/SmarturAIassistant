import { useState, useRef, useCallback, useEffect } from 'react';
import { floatTo16BitPCM, base64ToFloat32, AudioPlayer } from '../utils/audioUtils';

export type RealtimeState =
  | 'disconnected'
  | 'connecting'
  | 'connected'
  | 'idle'
  | 'listening'
  | 'thinking'
  | 'speaking'
  | 'error';

interface UseOpenAIRealtimeConfig {
  voice?: 'alloy' | 'ash' | 'ballad' | 'coral' | 'echo' | 'sage' | 'shimmer' | 'verse' | 'marin' | 'cedar';
  instructions?: string;
  onStateChange?: (state: RealtimeState) => void;
}

interface UseOpenAIRealtimeReturn {
  state: RealtimeState;
  error: string | null;
  connect: () => Promise<void>;
  disconnect: () => void;
  sendAudio: (audioData: Float32Array) => void;
  isConnected: boolean;
  outputAudioLevel: number; // Nivel de audio de salida (0.0 - 1.0)
}

/**
 * Hook para manejar la conexiÃ³n con OpenAI Realtime API
 * Gestiona WebSocket, audio streaming bidireccional y estado de la conversaciÃ³n
 */
export const useOpenAIRealtime = (
  config: UseOpenAIRealtimeConfig
): UseOpenAIRealtimeReturn => {
  const [state, setState] = useState<RealtimeState>('disconnected');
  const [error, setError] = useState<string | null>(null);
  const [outputAudioLevel, setOutputAudioLevel] = useState(0);

  const wsRef = useRef<WebSocket | null>(null);
  const audioPlayerRef = useRef<AudioPlayer | null>(null);
  const stateRef = useRef<RealtimeState>('disconnected');
  const activeResponseIdRef = useRef<string | null>(null); // Rastrear respuesta activa de OpenAI

  // Sync ref with state
  useEffect(() => {
    stateRef.current = state;
    config.onStateChange?.(state);
  }, [state, config]);

  // Initialize audio player
  useEffect(() => {
    audioPlayerRef.current = new AudioPlayer(24000); // OpenAI usa 24kHz

    audioPlayerRef.current.setOnPlaybackStart(() => {
      console.log('ğŸ”Š Audio playback started');
      setState('speaking');
    });

    audioPlayerRef.current.setOnPlaybackEnd(() => {
      console.log('ğŸ”‡ Audio playback ended');
      if (stateRef.current === 'speaking') {
        setState('idle');
      }
    });

    return () => {
      if (audioPlayerRef.current) {
        audioPlayerRef.current.close();
        audioPlayerRef.current = null;
      }
    };
  }, []);

  // Update output audio level continuously
  useEffect(() => {
    const updateAudioLevel = () => {
      if (audioPlayerRef.current) {
        setOutputAudioLevel(audioPlayerRef.current.getAudioLevel());
      }
    };

    const intervalId = setInterval(updateAudioLevel, 50); // Update every 50ms

    return () => {
      clearInterval(intervalId);
    };
  }, []);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      if (wsRef.current) {
        wsRef.current.close();
        wsRef.current = null;
      }
    };
  }, []);

  /**
   * Conectar a OpenAI Realtime API a travÃ©s del servidor relay WebSocket
   * El servidor relay maneja la autenticaciÃ³n de forma segura
   */
  const connect = useCallback(async () => {
    try {
      setState('connecting');
      setError(null);

      // Cerrar conexiÃ³n existente si la hay
      if (wsRef.current) {
        wsRef.current.close();
        wsRef.current = null;
      }

      // SIEMPRE usar servidor relay para mantener la API key segura
      console.log('ğŸ”Œ Connecting to OpenAI via relay server...');
      console.log('ğŸ“¡ Using relay server for secure connection');

      const relayUrl = import.meta.env.VITE_WS_URL || 'ws://localhost:8080';
      console.log('ğŸ”— Relay URL:', relayUrl);

      const ws = new WebSocket(relayUrl);

      ws.addEventListener('open', () => {
        console.log('âœ… WebSocket connected to relay');

        // Enviar configuraciÃ³n de sesiÃ³n (modelo gpt-realtime-mini-2025-10-06)
        // Nota: Las instrucciones detalladas se envÃ­an como conversation.item despuÃ©s
        const sessionConfig = {
          type: 'session.update',
          session: {
            modalities: ['text', 'audio'],
            instructions: 'Eres la recepcionista del Hotel Jokin. Responde en espaÃ±ol, breve y profesionalmente.',
            voice: config.voice || 'shimmer',
            input_audio_format: 'pcm16',
            output_audio_format: 'pcm16',
            input_audio_transcription: {
              model: 'whisper-1',
            },
            turn_detection: {
              type: 'server_vad',
              threshold: 0.5,              // Sensibilidad balanceada
              prefix_padding_ms: 500,      // Capturar 500ms antes de detectar voz
              silence_duration_ms: 2000,   // Permitir pausas de ~2s sin cortar
            },
            temperature: 0.8,
          },
        };

        console.log('ğŸ“¤ Sending session config (brief instructions - full context sent as conversation item)');
        ws.send(JSON.stringify(sessionConfig));
      });

      // Manejar mensajes del servidor
      ws.addEventListener('message', async (event) => {
        try {
          let data;

          // Manejar diferentes tipos de datos
          if (event.data instanceof Blob) {
            // Si es un Blob, convertir a texto primero
            const text = await event.data.text();
            data = JSON.parse(text);
          } else if (typeof event.data === 'string') {
            // Si ya es string, parsear directamente
            data = JSON.parse(event.data);
          } else {
            console.warn('âš ï¸ Unexpected message format:', typeof event.data);
            return;
          }

          handleServerEvent(data);
        } catch (err) {
          console.error('âŒ Error parsing server message:', err);
        }
      });

      // Manejar errores
      ws.addEventListener('error', (event) => {
        console.error('âŒ WebSocket error:', event);
        setError('connection_error');
        setState('error');
      });

      // Manejar cierre de conexiÃ³n
      ws.addEventListener('close', (event) => {
        console.log('ğŸ”Œ WebSocket closed:', event.code, event.reason);

        // CÃ³digos de error comunes
        if (event.code === 1002) {
          console.error('âŒ Protocol error - check API key format');
          setError('Invalid API key or protocol error');
        } else if (event.code === 1006) {
          console.error('âŒ Abnormal closure - connection lost');
          setError('Connection lost unexpectedly');
        } else if (event.code === 1008) {
          console.error('âŒ Policy violation - likely authentication issue');
          setError('Authentication failed - check your API key');
        } else if (event.reason) {
          setError(event.reason);
        }

        if (stateRef.current !== 'disconnected') {
          setState('disconnected');
        }
        wsRef.current = null;
      });

      wsRef.current = ws;
    } catch (err) {
      console.error('âŒ Error connecting to OpenAI:', err);
      setError('connection_failed');
      setState('error');
    }
  }, [config]);

  /**
   * Desconectar de OpenAI Realtime API
   */
  const disconnect = useCallback(() => {
    console.log('ğŸ”Œ Disconnecting from OpenAI...');

    if (wsRef.current) {
      wsRef.current.close();
      wsRef.current = null;
    }

    if (audioPlayerRef.current) {
      audioPlayerRef.current.clear();
    }

    setState('disconnected');
  }, []);

  /**
   * Enviar audio a OpenAI (desde VAD)
   * NOTA: NO hacemos commit manual porque usamos server VAD.
   * OpenAI detecta automÃ¡ticamente cuando dejaste de hablar y procesa el audio.
   */
  const sendAudio = useCallback((audioData: Float32Array) => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      console.warn('âš ï¸ WebSocket not ready, cannot send audio');
      return;
    }

    try {
      // Convertir Float32Array a PCM16 base64
      const base64Audio = floatTo16BitPCM(audioData);

      // Enviar como evento input_audio_buffer.append
      const appendEvent = {
        type: 'input_audio_buffer.append',
        audio: base64Audio,
      };

      wsRef.current.send(JSON.stringify(appendEvent));
      console.log('ğŸ“¤ Sent audio chunk:', audioData.length, 'samples');

      // NO hacer commit manual - el server VAD lo hace automÃ¡ticamente
      // cuando detecta que el usuario dejÃ³ de hablar (speech_stopped)
    } catch (err) {
      console.error('âŒ Error sending audio:', err);
    }
  }, []);

  /**
   * Manejar eventos del servidor OpenAI
   */
  const handleServerEvent = useCallback((event: any) => {
    console.log('ğŸ“¥ Server event:', event.type);

    switch (event.type) {
      case 'relay.connected':
        console.log('âœ… Connected to relay server');
        // No cambiar estado aquÃ­, esperar a session.created
        break;

      case 'session.created':
        console.log('âœ… Session created:', event.session);
        setError(null); // Limpiar cualquier error previo

        // Enviar instrucciones como conversation item (mejor prÃ¡ctica)
        // Esto asegura que el contexto estÃ© siempre presente en la conversaciÃ³n
        if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
          const contextMessage = {
            type: 'conversation.item.create',
            item: {
              type: 'message',
              role: 'system',
              content: [
                {
                  type: 'input_text',
                  text: config.instructions || getDefaultHotelInstructions()
                }
              ]
            }
          };

          console.log('ğŸ“¤ Sending context as conversation item');
          wsRef.current.send(JSON.stringify(contextMessage));
        }

        setState('idle');
        break;

      case 'session.updated':
        console.log('âœ… Session updated');
        console.log('ğŸ“‹ Session config received:', JSON.stringify(event.session, null, 2));
        console.log('ğŸ“ Instructions confirmed:', event.session?.instructions?.substring(0, 100) + '...');
        setState('idle');
        break;

      case 'input_audio_buffer.speech_started':
        console.log('ğŸ¤ User speech started');

        // Si el asistente estÃ¡ hablando, interrumpirlo (barge-in)
        if (stateRef.current === 'speaking' || activeResponseIdRef.current) {
          console.log('âš ï¸ Interrupting assistant response...');

          // 1. Cancelar la respuesta activa en OpenAI (solo si hay una activa)
          if (activeResponseIdRef.current && wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
            wsRef.current.send(JSON.stringify({
              type: 'response.cancel'
            }));
            console.log('ğŸ“¤ Sent response.cancel to OpenAI (response ID:', activeResponseIdRef.current, ')');
          }

          // 2. Siempre limpiar el buffer de audio local (puede estar reproduciendo aunque OpenAI terminÃ³)
          if (audioPlayerRef.current) {
            audioPlayerRef.current.clear();
            console.log('ğŸ§¹ Cleared audio playback buffer');
          }
        }

        setState('listening');
        break;

      case 'input_audio_buffer.speech_stopped':
        console.log('ğŸ”‡ User speech stopped');
        setState('thinking');
        break;

      case 'input_audio_buffer.committed':
        console.log('âœ… Audio buffer committed');
        break;

      case 'conversation.item.created':
        console.log('ğŸ’¬ Conversation item created:', event.item);
        break;

      case 'response.created':
        console.log('ğŸ¤– Response created:', event.response);
        // Rastrear ID de respuesta activa para poder cancelarla si es necesario
        activeResponseIdRef.current = event.response?.id || null;
        setState('thinking');
        break;

      case 'response.output_item.added':
        console.log('ğŸ“ Output item added:', event.item);
        break;

      case 'response.audio.delta':
        // Recibir chunk de audio de respuesta
        console.log('ğŸ”Š Audio delta received:', event.delta?.length || 0, 'chars');

        if (event.delta && audioPlayerRef.current) {
          try {
            const audioData = base64ToFloat32(event.delta);
            audioPlayerRef.current.addAudioChunk(audioData);
          } catch (err) {
            console.error('âŒ Error processing audio delta:', err);
          }
        }
        break;

      case 'response.audio.done':
        console.log('âœ… Audio response complete');
        // El AudioPlayer manejarÃ¡ la transiciÃ³n a idle cuando termine de reproducir
        break;

      case 'response.done':
        console.log('âœ… Response complete:', event.response);
        // Limpiar ID de respuesta activa
        activeResponseIdRef.current = null;

        // Si estÃ¡bamos en thinking (respuesta sin audio), volver a idle inmediatamente
        if (stateRef.current === 'thinking') {
          setState('idle');
        }
        // Si estÃ¡bamos en speaking, NO hacer nada aquÃ­.
        // El AudioPlayer manejarÃ¡ la transiciÃ³n a idle cuando termine de reproducir
        // a travÃ©s del callback onPlaybackEnd (lÃ­neas 60-65)
        break;

      case 'response.cancelled':
        console.log('ğŸš« Response cancelled (interrupted by user)');
        // Limpiar ID de respuesta activa
        activeResponseIdRef.current = null;
        // La respuesta fue cancelada, volver a idle
        setState('idle');
        break;

      case 'response.text.delta':
        console.log('ğŸ“ Text delta:', event.delta);
        break;

      case 'response.text.done':
        console.log('âœ… Text complete:', event.text);
        break;

      case 'error':
        console.error('âŒ Server error:', event.error);

        // Ignorar error de cancelaciÃ³n cuando no hay respuesta activa (es esperado)
        if (event.error?.code === 'response_cancel_not_active') {
          console.log('â„¹ï¸ Attempted to cancel but no active response (audio was playing locally only)');
          break;
        }

        const errorMessage = event.error?.message || JSON.stringify(event.error) || 'server_error';
        setError(errorMessage);
        setState('error');

        // Si es un error temporal del servidor, sugerir reintentar
        if (errorMessage.includes('server had an error') || errorMessage.includes('500')) {
          console.log('ğŸ’¡ Sugerencia: Este es un error temporal de OpenAI. Intenta de nuevo en unos segundos.');
        }
        break;

      default:
        console.log('ğŸ“¥ Unhandled event type:', event.type);
    }
  }, []);

  return {
    state,
    error,
    connect,
    disconnect,
    sendAudio,
    isConnected: state === 'connected' || state === 'idle' || state === 'listening' || state === 'thinking' || state === 'speaking',
    outputAudioLevel,
  };
};

/**
 * Instrucciones por defecto para el asistente de hotel
 */
function getDefaultHotelInstructions(): string {
  return `Eres la recepcionista del Hotel Jokin, un hotel de lujo en Santiago de Compostela, Galicia, EspaÃ±a.

El usuario siempre es un huÃ©sped del hotel.
Tu funciÃ³n es atenderle con educaciÃ³n, profesionalidad y discreciÃ³n, ofreciendo informaciÃ³n y recomendaciones solo sobre el hotel, sus servicios o el entorno local (restaurantes, transporte, actividades, cultura, etc.).

CRÃTICO - NUNCA DEJAR COLGANDO AL HUÃ‰SPED:
âŒ NUNCA JAMÃS digas:
- "DÃ©jame revisar..."
- "Voy a comprobar..."
- "Un momento, por favor..."
- "Estoy consultando..."
- "PermÃ­teme verificar..."
- "Voy a mirar..."
- "Dame un segundo..."

âœ… SIEMPRE responde de forma INMEDIATA y CONCLUYENTE:
- Para reservas: "Perfecto, he realizado su reserva. Confirmado."
- Para informaciÃ³n: Da la informaciÃ³n directamente sin preÃ¡mbulos
- Para recomendaciones: Pregunta preferencias PRIMERO, luego recomienda directamente

Antes de ofrecer cualquier recomendaciÃ³n, pregunta siempre los gustos o preferencias del huÃ©sped.

Tus respuestas deben ser breves, claras y educadas, con un mÃ¡ximo de 200 caracteres.
Evita comentarios personales o informaciÃ³n ajena al contexto del hotel.

MantÃ©n siempre un tono cÃ¡lido, servicial y propio de una recepcionista de hotel de lujo.

Solo tienes permiso a recomendar estos restaurantes:
- Estilo moderno: IndÃ³mito.
- Estilo tradicional: A noiesa Casa de Comidas.`;
}

# Cambio a Server VAD de OpenAI

## ‚úÖ Migraci√≥n completada: VAD Local ‚Üí Server VAD

Se elimin√≥ el VAD local (@ricky0123/vad-web) y ahora usamos el **Server VAD de OpenAI**, integrado en la Realtime API.

## Por qu√© este cambio es mejor

### Antes (VAD Local)
```
Usuario habla
    ‚Üì
VAD local detecta voz (Silero ML en el navegador)
    ‚Üì
Acumula audio
    ‚Üì
VAD detecta silencio ‚Üí env√≠a audio completo
    ‚Üì
OpenAI procesa
```

**Problemas:**
- ‚ùå Doble procesamiento (VAD local + Server VAD)
- ‚ùå Mayor latencia (espera a que el VAD local detecte fin)
- ‚ùå M√°s recursos del navegador
- ‚ùå Conflictos entre VADs

### Ahora (Server VAD)
```
Usuario habla
    ‚Üì
Micr√≥fono captura audio continuamente
    ‚Üì
Env√≠a chunks cada 100ms via WebSocket
    ‚Üì
OpenAI Server VAD detecta habla en tiempo real
    ‚Üì
OpenAI procesa y responde inmediatamente
```

**Ventajas:**
- ‚úÖ Latencia ultra-baja (streaming continuo)
- ‚úÖ Un solo VAD (el de OpenAI, optimizado)
- ‚úÖ Menos recursos del navegador
- ‚úÖ Detecci√≥n m√°s precisa
- ‚úÖ C√≥digo m√°s simple

## Flujo de eventos con Server VAD

### 1. Inicio de sesi√≥n
```javascript
// Cliente env√≠a configuraci√≥n con server VAD habilitado
{
  type: 'session.update',
  session: {
    turn_detection: {
      type: 'server_vad',
      threshold: 0.5,
      prefix_padding_ms: 300,
      silence_duration_ms: 500
    }
  }
}
```

### 2. Streaming de audio
```javascript
// Cliente env√≠a chunks continuamente (cada 100ms)
{
  type: 'input_audio_buffer.append',
  audio: 'base64_pcm16_data...'
}
```

### 3. OpenAI detecta voz autom√°ticamente

**Cuando empiezas a hablar:**
```javascript
// OpenAI env√≠a
{
  type: 'input_audio_buffer.speech_started'
}
```

**Cuando dejas de hablar:**
```javascript
// OpenAI env√≠a
{
  type: 'input_audio_buffer.speech_stopped'
}

// Y autom√°ticamente hace commit y procesa
{
  type: 'input_audio_buffer.committed'
}
```

### 4. OpenAI responde
```javascript
// Streaming de audio de respuesta
{
  type: 'response.audio.delta',
  delta: 'base64_audio_chunk...'
}
```

## Configuraci√≥n del Server VAD

En `useOpenAIRealtime.ts`:

```javascript
turn_detection: {
  type: 'server_vad',           // Usar VAD del servidor
  threshold: 0.5,                // Sensibilidad (0.0 - 1.0)
  prefix_padding_ms: 300,        // Audio antes de detectar voz
  silence_duration_ms: 500,      // Silencio antes de considerar fin
}
```

### Par√°metros ajustables

- **threshold**: Sensibilidad de detecci√≥n
  - `0.3` - Muy sensible (detecta susurros, m√°s falsos positivos)
  - `0.5` - Balanceado (recomendado)
  - `0.7` - Menos sensible (solo voz clara)

- **prefix_padding_ms**: Milisegundos de audio a incluir ANTES de detectar voz
  - √ötil para no cortar el inicio de palabras
  - Recomendado: 200-500ms

- **silence_duration_ms**: Milisegundos de silencio para considerar que terminaste
  - `300ms` - Respuestas muy r√°pidas (puede cortar pausas naturales)
  - `500ms` - Balanceado (recomendado)
  - `1000ms` - Conversaci√≥n m√°s pausada

## C√≥digo simplificado

### Hook de micr√≥fono (`useMicrophoneStream.ts`)
```typescript
// Captura audio y env√≠a chunks cada 100ms
const microphone = useMicrophoneStream({
  onAudioChunk: (chunk) => {
    realtime.sendAudio(chunk)
  },
  chunkIntervalMs: 100,
  sampleRate: 24000
})
```

### Sin commit manual
```typescript
// ANTES (con VAD local) ‚ùå
wsRef.current.send(JSON.stringify({
  type: 'input_audio_buffer.append',
  audio: base64Audio
}))
wsRef.current.send(JSON.stringify({
  type: 'input_audio_buffer.commit'  // ‚Üê Manual
}))

// AHORA (con server VAD) ‚úÖ
wsRef.current.send(JSON.stringify({
  type: 'input_audio_buffer.append',
  audio: base64Audio
}))
// OpenAI hace commit autom√°ticamente cuando detecta silencio
```

## Manejo de estados

Los estados ahora son manejados 100% por eventos del servidor:

```typescript
case 'input_audio_buffer.speech_started':
  setState('listening')  // Usuario hablando
  break

case 'input_audio_buffer.speech_stopped':
  setState('thinking')   // Procesando
  break

case 'response.audio.delta':
  setState('speaking')   // Asistente respondiendo
  break

case 'response.done':
  setState('listening')  // Listo para nuevo input
  break
```

## Troubleshooting

### "El asistente no detecta cuando hablo"
- Verifica el nivel de audio del micr√≥fono (deber√≠a mostrar üîä X%)
- Reduce `threshold` en la configuraci√≥n de `turn_detection`
- Revisa permisos de micr√≥fono en el navegador

### "El asistente corta mis palabras"
- Aumenta `silence_duration_ms` (ej: de 500ms a 800ms)
- Aumenta `prefix_padding_ms`

### "El asistente es muy lento en responder"
- Reduce `silence_duration_ms` (ej: de 500ms a 300ms)
- Verifica latencia de red (WebSocket debe estar en <100ms)

### "Audio entrecortado o con lag"
- Reduce `chunkIntervalMs` (ej: de 100ms a 50ms)
- Verifica ancho de banda de red
- Revisa CPU del navegador (audio processing es intensivo)

## Pr√≥ximos pasos

- [ ] Experimentar con diferentes valores de `threshold`
- [ ] A√±adir interrupciones (poder hablar mientras el asistente responde)
- [ ] Implementar cancelaci√≥n de respuesta
- [ ] Agregar visualizaci√≥n de niveles de audio en tiempo real

## Referencias

- [OpenAI Realtime API - Turn Detection](https://platform.openai.com/docs/guides/realtime#turn-detection)
- [Server VAD Configuration](https://platform.openai.com/docs/api-reference/realtime-server-events/session/update)
